{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn import utils\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Clean DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Data/df_train.pkl', 'rb') as f:\n",
    "    df_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Stop Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Data/stop_word_list.pkl', 'rb') as f:\n",
    "    stop_word_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec\n",
    "\n",
    "Many current NLP systems and techniques treat words as individual units - there is no notion of similarity between words, and they are represented as indices in a vocabulary.  These document representations lose the ordering of the words and they also ignore semantics of the words.  When it comes to evaluating texts, one of the most common fixed-length vectors is the bag-of-words.  \n",
    "\n",
    "Different from the bag-of-words models, word embeddings are representations of words in an N-dimensional vector space so that semantically similar (e.g. “king” — “monarch”) or semantically related (e.g. “bird” — “fly”) words come closer.  Word2Vec is a technique for learning high-quality word vectors that embeds words in a lower-dimensional vector space using a shallow neural network.  The result of this unsupervised framework is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings.\n",
    "\n",
    "Doc2Vec uses the same logic as word2vec, but applies this to a document level; it modifies the Word2Vec algorithm to unsupervised learning of continuous representations for larger blocks of text, such as sentences, paragraphs or entire documents.  There are two methods of implementing Doc2Vec: Distributed Memory (DM) and Distributed Bag of Words (DBOW).  DM attempts to predict a word given its previous words and paragraph vector. DBOW predicts a random group of words in a paragraph given only its paragraph vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the python library gensim, since it has a much more readable implementation of Doc2Vec.  I start by splitting all of the reviews, tag each word in the review with its respective index number, and append each of these tagged reviews to the tagged_documents list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_documents = []\n",
    "for indx, doc in enumerate(df_train[\"review\"].values):\n",
    "    tagged_documents.append(TaggedDocument([x for x in doc.split()], [indx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Tagged Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Data/tagged_documents.pkl', 'wb+') as f:\n",
    "    pickle.dump(tagged_documents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tagging the reviews, the data is ready for training and I can start modeling.  The dm parameter defines the training algorithm in Doc2Vec. dm=1 means ‘distributed memory’ (PV-DM) and dm=0 means ‘distributed bag of words’ (PV-DBOW).  The distributed memory model preserves the word order in a document whereas distributed bag of words uses the bag of words approach, which doesn’t preserve any word order.  I implemented the DM algorithm in two ways - one  which averages context vectors (dm_mean) and one which concatenates them (dm_concat). This was demonstrated in a [Doc2Vec tutorial](https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used an article entitled [An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation](https://arxiv.org/pdf/1607.05368.pdf) as guidance when deciding on the hyperparameters of the various Doc2Vec models as there does not appear to be a way to optimize parameters using gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I chose to limit my vector size to 300, since anything above that takes up a lot of memory and did not seem to improve my model performance.\n",
    "\n",
    "- If you set the negative parameter to > 0, negative sampling will be used. The number for negative specifies how many “noise words” will be drawn.  The gensim documentation  reccomends a range of 5-20.  \n",
    "\n",
    "- When hs is set to 0 and the negative parameter is > 0, negative sampling will be used. Due to the size of the dataset I am working with the neural network within Doc2Vec has a lot of weights, all of which would be updated by every one of the training samples.  Negative sampling addresses this by having each training sample only modify a small percentage of the weights.\n",
    "\n",
    "- Min_count ignores all words with total frequency lower than the set number.\n",
    "\n",
    "- Alpha indicates the initial learning rate of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "vec_size = 300\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, dbow_words=1, vector_size=vec_size, negative=5, hs=0, min_count=2, sample=0, \n",
    "             workers=cores)\n",
    "\n",
    "model_dm_mean = Doc2Vec(dm=1, dm_mean=1, vector_size=vec_size, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "                workers=cores, alpha=0.05, comment='alpha=0.05')\n",
    "\n",
    "model_dm_concat = Doc2Vec(dm=1, dm_concat=1, vector_size=vec_size, window=5, negative=5, hs=0, min_count=2, sample=0, \n",
    "                  workers=cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will set up a for loop which will scan and initialize the vocabulary for each of these models. The vocabulary is a dictionary **(accessible via model.wv.vocab)** of all of the unique words extracted from the training corpus along with the count **(e.g., model.wv.vocab['penalty'].count for counts for the word penalty)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d150,n5,w5,mc2,t4) vocabulary scanned & state initialized\n",
      "Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4) vocabulary scanned & state initialized\n",
      "Doc2Vec(dm/c,d150,n5,w5,mc2,t4) vocabulary scanned & state initialized\n"
     ]
    }
   ],
   "source": [
    "models = [(model_dbow, 'model_dbow'), (model_dm_mean, 'model_dm_mean'), (model_dm_concat, 'model_dm_concat')]\n",
    "\n",
    "for model in models:\n",
    "    model[0].build_vocab(tagged_documents)\n",
    "    print(\"%s vocabulary scanned & state initialized\" % model[0])\n",
    "    \n",
    "models_by_name = OrderedDict((str(model[1]), model[0]) for model in models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the models I shuffle the corpus before each pass because the native corpus is organized in a stacked fashion where all the negative sentiment documents come first and then are followed by the positive sentiment documents.  A shuffle will break up these groupings and should lead to better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 1 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 2 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 3 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 4 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 5 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 6 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 7 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 8 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 9 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 10 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 11 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 12 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 13 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 14 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 15 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 16 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 17 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 18 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 19 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 20 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 21 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 22 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 23 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 24 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 25 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 26 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 27 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 28 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 29 Model: Doc2Vec(dbow+w,d150,n5,w5,mc2,t4)\n",
      "Epoch: 0 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 1 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 2 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 3 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 4 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 5 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 6 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 7 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 8 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 9 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 10 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 11 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 12 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 13 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 14 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 15 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 16 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 17 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 18 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 19 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 20 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 21 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 22 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 23 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 24 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 25 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 26 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 27 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 28 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 29 Model: Doc2Vec(\"alpha=0.05\",dm/m,d150,n5,w10,mc2,t4)\n",
      "Epoch: 0 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 1 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 2 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 3 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 4 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 5 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 6 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 7 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 8 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 9 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 10 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 11 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 12 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 13 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 14 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 15 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 16 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 17 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 18 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 19 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 20 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 21 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 22 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 23 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 24 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 25 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 26 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 27 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 28 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n",
      "Epoch: 29 Model: Doc2Vec(dm/c,d150,n5,w5,mc2,t4)\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    for epoch in range(30):\n",
    "        print('Epoch: {0}'.format(epoch), 'Model: %s' % (model[0]))\n",
    "        model[0].train(utils.shuffle(tagged_documents), total_examples=len(tagged_documents), epochs=1)\n",
    "        model[0].alpha -= 0.002\n",
    "        model[0].min_alpha = model[0].alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Models/model_dbow.pkl', 'wb+') as f:\n",
    "    pickle.dump(model_dbow, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Models/model_dm_mean.pkl', 'wb+') as f:\n",
    "    pickle.dump(model_dm_mean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Models/model_dm_concat.pkl', 'wb+') as f:\n",
    "    pickle.dump(model_dm_concat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = {}\n",
    "\n",
    "for model in models:\n",
    "    X[model[1]] = np.zeros((df_train.shape[0], vec_size))\n",
    "    for i in range(df_train.shape[0]):\n",
    "        X[model[1]][i] = model[0].docvecs[i]  \n",
    "        \n",
    "#Keys are just the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df_train['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export X and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Data/X_doc2vec.pkl', 'wb+') as f:\n",
    "    pickle.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Data/y_doc2vec.pkl', 'wb+') as f:\n",
    "    pickle.dump(y, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
