{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twython import Twython  \n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twython import Twython  \n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#pass security information to variables\n",
    "consumer_key = \"XVFBqWVkhDlRBFz2f7CbvIiXI\"\n",
    "consumer_secret = \"S4BO3WQpFHRX3q0vOTGZvre3TIPyxtt6w3WTPAexqNtii5fsIw\"\n",
    "access_key = \"1034835017371770880-b4eFRilbAVFpPwA5tTXRp2JBhr6rWK\"\n",
    "access_secret = \"VWAg8gzTHPXJxfY9GwGkcI2iuplAtw5NxhhnQPcbNwKRr\"\n",
    "\n",
    "# Instantiate an object\n",
    "apple_tweets = Twython(consumer_key, consumer_secret)\n",
    "\n",
    "# Create our query\n",
    "query = {'q': '#FirstMan',  \n",
    "        'result_type': 'recent',\n",
    "        'lang': 'en',\n",
    "        'count': 100,\n",
    "        'tweet_mode':'extended'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 out 150 pages\n",
      "Processing 1 out 150 pages\n",
      "Processing 2 out 150 pages\n",
      "Processing 3 out 150 pages\n",
      "Processing 4 out 150 pages\n",
      "Processing 5 out 150 pages\n",
      "Processing 6 out 150 pages\n",
      "Processing 7 out 150 pages\n",
      "Processing 8 out 150 pages\n",
      "Processing 9 out 150 pages\n",
      "Processing 10 out 150 pages\n",
      "Processing 11 out 150 pages\n",
      "Processing 12 out 150 pages\n",
      "Processing 13 out 150 pages\n",
      "Processing 14 out 150 pages\n",
      "Processing 15 out 150 pages\n",
      "Processing 16 out 150 pages\n",
      "Processing 17 out 150 pages\n",
      "Processing 18 out 150 pages\n",
      "Processing 19 out 150 pages\n",
      "Processing 20 out 150 pages\n",
      "Processing 21 out 150 pages\n",
      "Processing 22 out 150 pages\n",
      "Processing 23 out 150 pages\n",
      "Processing 24 out 150 pages\n",
      "Processing 25 out 150 pages\n",
      "Processing 26 out 150 pages\n",
      "Processing 27 out 150 pages\n",
      "Processing 28 out 150 pages\n",
      "Processing 29 out 150 pages\n",
      "Processing 30 out 150 pages\n",
      "Processing 31 out 150 pages\n",
      "Processing 32 out 150 pages\n",
      "Processing 33 out 150 pages\n",
      "Processing 34 out 150 pages\n",
      "Processing 35 out 150 pages\n",
      "Processing 36 out 150 pages\n",
      "Processing 37 out 150 pages\n",
      "Processing 38 out 150 pages\n",
      "Processing 39 out 150 pages\n",
      "Processing 40 out 150 pages\n",
      "Processing 41 out 150 pages\n",
      "Processing 42 out 150 pages\n",
      "Processing 43 out 150 pages\n",
      "Processing 44 out 150 pages\n",
      "Processing 45 out 150 pages\n",
      "Processing 46 out 150 pages\n",
      "Processing 47 out 150 pages\n",
      "Processing 48 out 150 pages\n",
      "Processing 49 out 150 pages\n",
      "Processing 50 out 150 pages\n",
      "Processing 51 out 150 pages\n",
      "Processing 52 out 150 pages\n",
      "Processing 53 out 150 pages\n",
      "Processing 54 out 150 pages\n",
      "Processing 55 out 150 pages\n",
      "Processing 56 out 150 pages\n",
      "Processing 57 out 150 pages\n",
      "Processing 58 out 150 pages\n",
      "Processing 59 out 150 pages\n",
      "Processing 60 out 150 pages\n",
      "Processing 61 out 150 pages\n",
      "Processing 62 out 150 pages\n",
      "Processing 63 out 150 pages\n",
      "Processing 64 out 150 pages\n",
      "Processing 65 out 150 pages\n",
      "Processing 66 out 150 pages\n",
      "Processing 67 out 150 pages\n",
      "Processing 68 out 150 pages\n",
      "Processing 69 out 150 pages\n",
      "Processing 70 out 150 pages\n",
      "Processing 71 out 150 pages\n",
      "Processing 72 out 150 pages\n",
      "Processing 73 out 150 pages\n",
      "Processing 74 out 150 pages\n",
      "Processing 75 out 150 pages\n",
      "Processing 76 out 150 pages\n",
      "Processing 77 out 150 pages\n",
      "Processing 78 out 150 pages\n",
      "Processing 79 out 150 pages\n",
      "Processing 80 out 150 pages\n",
      "Processing 81 out 150 pages\n",
      "Processing 82 out 150 pages\n",
      "Processing 83 out 150 pages\n",
      "Processing 84 out 150 pages\n",
      "Processing 85 out 150 pages\n",
      "Processing 86 out 150 pages\n",
      "Processing 87 out 150 pages\n",
      "Processing 88 out 150 pages\n",
      "Processing 89 out 150 pages\n",
      "Processing 90 out 150 pages\n",
      "Processing 91 out 150 pages\n",
      "Processing 92 out 150 pages\n",
      "Processing 93 out 150 pages\n",
      "Processing 94 out 150 pages\n",
      "Processing 95 out 150 pages\n",
      "Processing 96 out 150 pages\n",
      "Processing 97 out 150 pages\n",
      "Processing 98 out 150 pages\n",
      "Processing 99 out 150 pages\n",
      "Processing 100 out 150 pages\n",
      "Processing 101 out 150 pages\n",
      "Processing 102 out 150 pages\n",
      "Processing 103 out 150 pages\n",
      "Processing 104 out 150 pages\n",
      "Processing 105 out 150 pages\n",
      "Processing 106 out 150 pages\n",
      "Processing 107 out 150 pages\n",
      "Processing 108 out 150 pages\n",
      "Processing 109 out 150 pages\n",
      "Processing 110 out 150 pages\n",
      "Processing 111 out 150 pages\n",
      "Processing 112 out 150 pages\n",
      "Processing 113 out 150 pages\n",
      "Processing 114 out 150 pages\n",
      "Processing 115 out 150 pages\n",
      "Processing 116 out 150 pages\n",
      "Processing 117 out 150 pages\n",
      "Processing 118 out 150 pages\n",
      "Processing 119 out 150 pages\n",
      "Processing 120 out 150 pages\n",
      "Processing 121 out 150 pages\n",
      "Processing 122 out 150 pages\n",
      "Processing 123 out 150 pages\n",
      "Processing 124 out 150 pages\n",
      "Processing 125 out 150 pages\n",
      "Processing 126 out 150 pages\n",
      "Processing 127 out 150 pages\n",
      "Processing 128 out 150 pages\n",
      "Processing 129 out 150 pages\n",
      "Processing 130 out 150 pages\n",
      "Processing 131 out 150 pages\n",
      "Processing 132 out 150 pages\n",
      "Processing 133 out 150 pages\n",
      "Processing 134 out 150 pages\n",
      "Processing 135 out 150 pages\n",
      "Processing 136 out 150 pages\n",
      "Processing 137 out 150 pages\n",
      "Processing 138 out 150 pages\n",
      "Processing 139 out 150 pages\n",
      "Processing 140 out 150 pages\n",
      "Processing 141 out 150 pages\n",
      "Processing 142 out 150 pages\n",
      "Processing 143 out 150 pages\n",
      "Processing 144 out 150 pages\n",
      "Processing 145 out 150 pages\n",
      "Processing 146 out 150 pages\n",
      "Processing 147 out 150 pages\n",
      "Processing 148 out 150 pages\n",
      "Processing 149 out 150 pages\n"
     ]
    }
   ],
   "source": [
    "dict_ = {'user': [], 'date': [], 'text': []}  \n",
    "npages = 150\n",
    "for i in range(npages):\n",
    "    # Search tweets\n",
    "    print('Processing {0} out {1} pages'.format(i,npages))\n",
    "    results = apple_tweets.search(**query)\n",
    "    \n",
    "    \n",
    "    for status in results['statuses']:  \n",
    "        dict_['user'].append(status['user']['screen_name'])\n",
    "        dict_['date'].append(status['created_at'])\n",
    "        if status.get('retweeted_status'):\n",
    "            dict_['text'].append(status['retweeted_status']['full_text'])\n",
    "        else:\n",
    "            dict_['text'].append(status['full_text'])\n",
    "            \n",
    "    query['max_id'] = results['statuses'][-1]['id_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure data in a pandas DataFrame for easier manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame(dict_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert date column into datetime format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df['date'] = pd.to_datetime(tweets_df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df.drop_duplicates('text', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reset Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df.drop(columns='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5732, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Data/tweets_df', 'wb+') as f:\n",
    "    pickle.dump(tweets_df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
