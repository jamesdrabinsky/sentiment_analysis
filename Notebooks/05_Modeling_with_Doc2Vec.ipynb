{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Data/X_doc2vec.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Data/y_doc2vec.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Doc2Vec Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Models/model_dbow.pkl', 'rb') as f:\n",
    "    model_dbow = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Models/model_dm_mean.pkl', 'rb') as f:\n",
    "    model_dm_mean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Models/model_dm_concat.pkl', 'rb') as f:\n",
    "    model_dm_concat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [(model_dbow, 'model_dbow'), (model_dm_mean, 'model_dm_mean'), (model_dm_concat, 'model_dm_concat')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the vectors I got from the Doc2Vec models, I will fit a number of differt classifiers and evaluate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Naive_Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bern_nb_model_func(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "    bern_nb = BernoulliNB()\n",
    "    bern_nb.fit(X_train, y_train)\n",
    "    print(\"cross-validated train scores:\", cross_val_score(bern_nb, X_train, y_train, cv=3))\n",
    "    print(\"cross-validated test scores:\", cross_val_score(bern_nb, X_test, y_test, cv=3))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1038d0240>, 'model_dbow')\n",
      "cross-validated train scores: [0.80086719 0.80195952 0.79518072]\n",
      "cross-validated test scores: [0.80684008 0.79576108 0.78640309]\n",
      "---\n",
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1a254f45c0>, 'model_dm_mean')\n",
      "cross-validated train scores: [0.79058937 0.7902345  0.78361446]\n",
      "cross-validated test scores: [0.80684008 0.77842004 0.79074253]\n",
      "---\n",
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1038d04a8>, 'model_dm_concat')\n",
      "cross-validated train scores: [0.58294524 0.59058786 0.5913253 ]\n",
      "cross-validated test scores: [0.5977842  0.57996146 0.58968177]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    bern_nb_model_func(X[model[1]], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gb_model_func(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "    gb = GaussianNB()\n",
    "    gb.fit(X_train, y_train)\n",
    "    print(\"cross-validated train scores:\", cross_val_score(gb, X_train, y_train, cv=3))\n",
    "    print(\"cross-validated test scores:\", cross_val_score(gb, X_test, y_test, cv=3))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1038d0240>, 'model_dbow')\n",
      "cross-validated train scores: [0.84278144 0.83841953 0.84048193]\n",
      "cross-validated test scores: [0.83863198 0.83044316 0.82401157]\n",
      "---\n",
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1a254f45c0>, 'model_dm_mean')\n",
      "cross-validated train scores: [0.79058937 0.80228076 0.8       ]\n",
      "cross-validated test scores: [0.75626204 0.76974952 0.7656702 ]\n",
      "---\n",
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1038d04a8>, 'model_dm_concat')\n",
      "cross-validated train scores: [0.58069696 0.58207517 0.58891566]\n",
      "cross-validated test scores: [0.56840077 0.58766859 0.57473481]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    gb_model_func(X[model[1]], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_model_func(X,y):\n",
    "    lr = LogisticRegression()\n",
    "    pipe = Pipeline([\n",
    "        ('lr', lr)\n",
    "    ])\n",
    "    \n",
    "    params = {\n",
    "        'lr__penalty': ['l1'],\n",
    "        'lr__C': [0.5]\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "    gs_d2v_lr = GridSearchCV(pipe, param_grid=params, cv=2)\n",
    "    gs_d2v_lr.fit(X_train, y_train)\n",
    "    print('train score:', gs_d2v_lr.score(X_train, y_train))\n",
    "    print('test score:', gs_d2v_lr.score(X_test, y_test))\n",
    "    print('best score:', gs_d2v_lr.best_score_)\n",
    "    print('best params:', gs_d2v_lr.best_params_)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1038d0240>, 'model_dbow')\n",
      "train score: 0.8792161901702538\n",
      "test score: 0.8784131063283007\n",
      "best score: 0.8746118428097227\n",
      "best params: {'lr__C': 0.5, 'lr__penalty': 'l1'}\n",
      "---\n",
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1a254f45c0>, 'model_dm_mean')\n",
      "train score: 0.8543205910697077\n",
      "test score: 0.8597815611949887\n",
      "best score: 0.8492879323268016\n",
      "best params: {'lr__C': 0.5, 'lr__penalty': 'l1'}\n",
      "---\n",
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1038d04a8>, 'model_dm_concat')\n",
      "train score: 0.6685405289645572\n",
      "test score: 0.6723417924831352\n",
      "best score: 0.6604561516222294\n",
      "best params: {'lr__C': 0.5, 'lr__penalty': 'l1'}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    lr_model_func(X[model[1]], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rf_model_func(X,y):\n",
    "    rf = RandomForestClassifier()\n",
    "    pipe = Pipeline([\n",
    "        ('rf', rf)\n",
    "    ])\n",
    "    \n",
    "    params = {\n",
    "        'rf__n_estimators': [750],\n",
    "        'rf__max_features': ['log2', 'sqrt']\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "    gs_d2v_rf = GridSearchCV(pipe, param_grid=params, cv=2)\n",
    "    gs_d2v_rf.fit(X_train, y_train)\n",
    "    print('train score:', gs_d2v_rf.score(X_train, y_train))\n",
    "    print('test score:', gs_d2v_rf.score(X_test, y_test))\n",
    "    print('best score:', gs_d2v_rf.best_score_)\n",
    "    print('best params:', gs_d2v_rf.best_params_)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1038d0240>, 'model_dbow')\n",
      "train score: 1.0\n",
      "test score: 0.8560873755220045\n",
      "best score: 0.8515901060070671\n",
      "best params: {'rf__max_features': 'log2', 'rf__n_estimators': 750}\n",
      "---\n",
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1a254f45c0>, 'model_dm_mean')\n",
      "train score: 1.0\n",
      "test score: 0.8311917764214584\n",
      "best score: 0.8238569439982868\n",
      "best params: {'rf__max_features': 'log2', 'rf__n_estimators': 750}\n",
      "---\n",
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1038d04a8>, 'model_dm_concat')\n",
      "train score: 1.0\n",
      "test score: 0.6429489238676518\n",
      "best score: 0.6283863368669023\n",
      "best params: {'rf__max_features': 'sqrt', 'rf__n_estimators': 750}\n",
      "---\n",
      "CPU times: user 22min 36s, sys: 10.3 s, total: 22min 46s\n",
      "Wall time: 48min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    rf_model_func(X[model[1]], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_model_func(X,y):\n",
    "    grad = GradientBoostingClassifier()\n",
    "    pipe = Pipeline([\n",
    "        ('grad', grad)\n",
    "    ])\n",
    "    \n",
    "    params = {\n",
    "        'grad__n_estimators': [1000],\n",
    "        'grad__max_features': ['log2']\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "    gs_d2v_grad = GridSearchCV(pipe, param_grid=params, cv=2)\n",
    "    gs_d2v_grad.fit(X_train, y_train)\n",
    "    print('train score:', gs_d2v_grad.score(X_train, y_train))\n",
    "    print('test score:', gs_d2v_grad.score(X_test, y_test))\n",
    "    print('best score:', gs_d2v_grad.best_score_)\n",
    "    print('best params:', gs_d2v_grad.best_params_)\n",
    "#     print('classification report': \n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1038d0240>, 'model_dbow')\n",
      "train score: 0.9561516222293608\n",
      "test score: 0.8766463218760039\n",
      "best score: 0.8671163936181604\n",
      "best params: {'grad__max_features': 'log2', 'grad__n_estimators': 1000}\n",
      "---\n",
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1a254f45c0>, 'model_dm_mean')\n",
      "train score: 0.9406253346182675\n",
      "test score: 0.847414070028911\n",
      "best score: 0.8393296926865832\n",
      "best params: {'grad__max_features': 'log2', 'grad__n_estimators': 1000}\n",
      "---\n",
      "(<gensim.models.doc2vec.Doc2Vec object at 0x1038d04a8>, 'model_dm_concat')\n",
      "train score: 0.8335474890245208\n",
      "test score: 0.6567619659492451\n",
      "best score: 0.6375950315879645\n",
      "best params: {'grad__max_features': 'log2', 'grad__n_estimators': 1000}\n",
      "---\n",
      "CPU times: user 2min 27s, sys: 935 ms, total: 2min 28s\n",
      "Wall time: 2min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    grad_model_func(X[model[1]], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When pairing the Doc2Vec models with classifiers, the two pairings that produced the best scores were Doc2Vec DBOW with logistic regression and Doc2Vec DBOW with gradient boost.  I decided to use the gradient boost classifier in my production model.  I made this decision because gradient boosting is an ensemble technique which means it has many different predictors trying to predict the same target variable, and as a boosting method, it adds one classifier at a time so that the next classifier is trained to improve the already trained ensemble.  Logistic regression is a linear classifier and is not equipped to make the types of decision that gradient boosting does.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
